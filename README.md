<a name="top"></a>

# 95-865: Unstructured Data Analytics

## Table of Contents
1. 
2. 
3. 
4. 
5. 
6. 
7. [Clustering](#7-clustering)

---

[Back to Top](#)











## 7. Clustering



### Overview

- **Clustering** is a method of unsupervised learning, a type of machine learning where the system learns to **identify patterns without prior labeling of the data**.

- Clustering methods aim to group together data points that are "similar" into "clusters", while having different clusters be "dissimilar".
    - Similarity is inversely related to distance (two points being more similar $\rightarrow$ closer in distance)
    - Use **Euclidean distance** between feature vectors

- Clustering structure often occurs
    - Crime happens more often in specific spots
    - Users share similar tastes in a recommendation system
    - 2-D t-SNE plot of handwritten digit images shows clumps that correspond to real digits


---


### Drug Consumption Data
Source: https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified

#### Demo


---

### Similarity/Distance Functions

1. **Euclidean distance** between feature vectors

2. **Levenshtein distance** (edit distance): the minimum number of single-letter insertions, deletions, or substitutions required to convert one string into another.
    - `kitten` and `sitting` has a Levenshtein distance of 3


---


### Effectiveness Assessment of Similarity/Distance Functions

- Step 1: Select a data point, which can be chosen randomly or deliberately.

- Step 2: Compute the similarity or distance between the selected data point and all other data points in the dataset and sort the data points based on their similarity or distance, from most similar to least similar (or smallest distance to largest).

- Step 3: **Manually examine** the data points that are most similar or closest to the selected point by inspecting their raw data.
    - <span style="color:red;">Similarity/distance functions is likely not good if the most similar/closest points are not interpretable.</span>


---



### Clustering Methods


| Generative Models | Hierarchical Clustering |
|-------------------|-------------------------|
| 1. Pretend data generated by specific model with parameters. <br> 2. Learn the parameters ("fit model to data"). <br> 3. Use fitted model to determine cluster assignments. | **Top-down**: Start with everything in 1 cluster and decide on how to recursively split. <br> **Bottom-up**: Start with everything in $n$ cluster and decide on how to iteratively merge. |
| - | <span style="color:red;">Requires certain termination criteria.</span> |





### K-Means Clustering

1. **Initialization**: Start by selecting $k$ initial centroids randomly.
    - One common approach is to randomly choose $k$ data points from the dataset as the initial centroids.

2. **Assignment**: Assign each data point to the nearest centroid.
    - The most common distance metric used is the Euclidean distance.

3. **Update**: Recalculate the centroids of the clusters by taking the mean of all the data points assigned to each cluster.
    - The mean (center of mass) becomes the new centroid of each cluster.

4. **Iteration**: Repeat the Assignment and Update steps until one of the following conditions is met:
    - The centroids do not change (or below a certain threshold), indicating convergence.
    - The assignments of data points to clusters remain the same between iterations.
    - A predefined number of iterations has been reached.

5. **Termination**: Return the centroids of the clusters and the assignments of each data point to a cluster.


---



### K-Means++

- K-Means++ uses a **weighted probability distribution** for the initialization step to improve the convergence of the K-Means algorithm, leading to better clustering outcomes and faster convergence.

- After choosing the first centroid randomly, K-Means++ selects subsequent centroids from the remaining data points with a probability proportional to the square of the distance from each point to the nearest existing centroid.
    - <span style="color:red;">This step biases the selection towards points that are the furthest from the existing centroids, aiming to spread out the initial centroids.</span>


---


### Gaussian Mixture Model (GMM)

- GMM is a **probabilistic** (not deterministic) model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. 

- Data Assignment (Soft Clustering): Given a new data point, a GMM can calculate the probability of it belonging to each of the Gaussian components in the mixture.

#### General Case
- GMM is the sum of $k$ different $d$-dimensional Gaussian distributions.
    - The overall probability distribution looks like $k$ mountains
    - Each mountain corresponds to a different cluster
    - Different mountains can have different peak heights
    - Different mountains can have different ellipse shapes that captures correlation/covariance information


#### Learning a GMM

- Step 0: Guess $k$

- Step 1: Guess cluster probabilities, means, and covariances

Repeat until convergence:

- Compute probability of each point being in each of the $k$ clusters

- Update cluster probabilities, means, and covariances accounting for probabilities of each point belonging to each of the clusters


#### Limitations
1. In reality, data points are unlikely generated the same way!
2. In reality, data points might not be independent!


#### Caveat
"All models are wrong, but some are useful."

- Models are approximations/simplifications of the reality.
- Some models provide insights, make predictions, or enable decisions that are sufficiently accurate for practical purposes.