<a name="top"></a>

# 95-865: Unstructured Data Analytics

## Table of Contents
1. 
2. 
3. 
4. 
5. 
6. 
7. [Clustering](#7-clustering)

---

[Back to Top](#)











## 7. Clustering



### Overview

- **Clustering** is a method of unsupervised learning, a type of machine learning where the system learns to **identify patterns without prior labeling of the data**.

- Clustering methods aim to group together data points that are "similar" into "clusters", while having different clusters be "dissimilar".
    - Similarity is inversely related to distance (two points being more similar $\rightarrow$ closer in distance)
    - Use **Euclidean distance** between feature vectors

- Clustering structure often occurs
    - 2-D t-SNE plot of handwritten digit images shows clumps that correspond to real digits
    - Crime might happen more often in specific hot spots



---


### Drug Consumption Data
Source: https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified

#### Demo


---

### Similarity/Distance Functions

1. **Euclidean distance** between feature vectors

2. **Levenshtein distance** (edit distance): the minimum number of single-letter insertions, deletions, or substitutions required to convert one string into another.
    - `kitten` and `sitting` has a Levenshtein distance of 3


---


### Effectiveness Assessment of Similarity/Distance Functions

- Step 1: Select a data point, which can be chosen randomly or deliberately.

- Step 2: Compute the similarity or distance between the selected data point and all other data points in the dataset and sort the data points based on their similarity or distance, from most similar to least similar (or smallest distance to largest).

- Step 3: **Manually examine** the data points that are most similar or closest to the selected point by inspecting their raw data.
    - <span style="color:red;">Similarity/distance functions is likely not good if the most similar/closest points are not interpretable.</span>


---



### Clustering Methods


| Generative Models | Hierarchical Clustering |
|-------------------|-------------------------|
| 1. Pretend data generated by specific model with parameters. <br> 2. Learn the parameters ("fit model to data"). <br> 3. Use fitted model to determine cluster assignments. | **Top-down**: Start with everything in 1 cluster and decide on how to recursively split. <br> **Bottom-up**: Start with everything in n cluster and decide on how to iteratively merge. |
| - | <span style="color:red;">Requires certain termination criteria.</span> |





### K-Means Clustering

1. **Initialization**: Start by selecting $k$ initial centroids randomly.
    - One common approach is to randomly choose $k$ data points from the dataset as the initial centroids.

2. **Assignment**: Assign each data point to the nearest centroid.
    - The most common distance metric used is the Euclidean distance.

3. **Update**: Recalculate the centroids of the clusters by taking the mean of all the data points assigned to each cluster.
    - The mean (center of mass) becomes the new centroid of each cluster.

4. **Iteration**: Repeat the Assignment and Update steps until one of the following conditions is met:
    - The centroids do not change (or below a certain threshold), indicating convergence.
    - The assignments of data points to clusters remain the same between iterations.
    - A predefined number of iterations has been reached.

5. **Termination**: Return the centroids of the clusters and the assignments of each data point to a cluster.


---



### K-Means++ 
Aimss to improve the convergence of the K-Means algorithm by carefully choosing the initial centroids.


Probability decays as the distance decreases.



### Gaussian Mixture Model (GMM)





### Caveat
All the models are wrong, rather approximation of the reality.
